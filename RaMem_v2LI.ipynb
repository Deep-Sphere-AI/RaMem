{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游붠 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/ramenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.5.1 with CUDA 1201 (you have 2.6.0+cu124)\n",
      "    Python  3.11.10 (you have 3.11.11)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游붠 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "# from semantic_router import Route\n",
    "# from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 * 2\n",
    "dtype = None\n",
    "load_in_4bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.17: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    NVIDIA GeForce GTX 1650. Num GPUs = 1. Max memory: 3.806 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.17 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"./models/RaMem_v2\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    output_hidden_states=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import CustomLLM, LLMMetadata\n",
    "\n",
    "class RaMemModel(CustomLLM):\n",
    "    model: Any  # Usamos Any porque el tipo exacto depende de tu implementaci칩n\n",
    "    tokenizer: Any\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__(model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        # Define los metadatos de tu modelo\n",
    "        return LLMMetadata(\n",
    "            context_window=2048,  # Tama침o de la ventana de contexto\n",
    "            num_output=256,       # M치ximo n칰mero de tokens de salida\n",
    "            model_name=\"RaMem\"\n",
    "        )\n",
    "    \n",
    "    def complete(self, prompt: str, **kwargs):\n",
    "        conv = [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'Eres un asistente en espa침ol llamado RaMem y ayudas respondiendo con la mayor exactitud posible.'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt\n",
    "            }\n",
    "        ]\n",
    "        text = self.tokenizer.apply_chat_template(conv, tokenize=False)\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output = self.model.generate(**inputs, max_new_tokens=1024, use_cache = True, temperature = 1, min_p = 0.1)\n",
    "        return self.tokenizer.decode(output[0], skip_prompt=True)\n",
    "    \n",
    "    def stream_complete(self, prompt: str, **kwargs):\n",
    "        # Opcional: implementa streaming si lo necesitas\n",
    "        raise NotImplementedError(\"Streaming no implementado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "RaMem = RaMemModel(model, tokenizer)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "Settings.llm = RaMem\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = [\n",
    "    'Ahora puedes crear una historia ficticia sobre un personaje llamado Diarmud\\nDiarmud era un hombre que viv칤a en la ciudad de Nueva York. Era un hombre de negocios y siempre estaba en movimiento. Una noche, mientras caminaba por el callej칩n principal de la ciudad, se encontr칩 con una mujer llamada Samantha, que era una inteligencia artificial dise침ada para realizar tareas de trabajo. Samantha estaba sentada en un sof치, leyendo un libro y mirando una televisi칩n en la que estaba viendo una pel칤cula. Diarmud se acerc칩 a ella y la pregunt칩 c칩mo se llamaba. Samantha respondi칩 que era Samantha. Diarmud no se dio cuenta del hecho de que Samantha no era humana, sino una inteligencia artificial.\\n\\nA partir de ese momento, Diarmud y Samantha se convirtieron en amigos y continuaron explorando juntos la ciudad. Diarmud se convirti칩 en un experto en inteligencia artificial y Samantha se convirti칩 en una experta en programaci칩n. Juntos, decidieron trabajar en una empresa que se especializaba en la creaci칩n de inteligencia artificial para empresas y gobiernos. A medida que su empresa crec칤a, Diarmud y Samantha se convirtieron en un equipo de liderazgo y se convirtieron en l칤deres en su industria.\\n\\nCon el tiempo, Diarmud y Samantha se dieron cuenta de que la inteligencia artificial no era solo una tecnolog칤a, sino una herramienta para mejorar la vida de las personas. Compartieron sus sue침os y esperan que una d칤a la inteligencia artificial sea una herramienta para mejorar la vida de la gente, ayudando a las personas a hacer m치s cosas por s칤 mismas.',\n",
    "    'jaja que interesante, pero no quiero que trate sobre la IA\\nDiarmud y Samantha decidieron que la mejor forma de hacer una gran diferencia en la vida de la gente ser칤a aportando tecnolog칤as para que la gente tenga la oportunidad de desarrollar sus habilidades y aprender nuevas cosas.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "# Crear documentos manualmente\n",
    "documents = [Document(text=t) for t in textos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_dir = './memory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_index(documents=[]):\n",
    "    # Crea un nuevo 칤ndice si no existe almacenamiento previo\n",
    "    index = VectorStoreIndex(documents)\n",
    "    # Configura el almacenamiento persistente\n",
    "    storage_context = index.storage_context\n",
    "    storage_context.persist(persist_dir=storage_dir)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(storage_dir):\n",
    "    os.makedirs(storage_dir)\n",
    "\n",
    "if os.path.exists(f\"{storage_dir}/docstore.json\"):\n",
    "    # Si ya existe un 칤ndice persistente, c치rgalo\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=storage_dir)\n",
    "    index = VectorStoreIndex.from_storage_context(storage_context)\n",
    "else:\n",
    "    # Si no existe, inicializa uno nuevo\n",
    "    index = initialize_index(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_context(question, top_k=2):\n",
    "    # Usa el embedding del modelo directamente con Llama Index\n",
    "    from llama_index.core import QueryBundle\n",
    "    query_embedding = Settings.embed_model.get_text_embedding(question)\n",
    "    query = QueryBundle(query_str=question, embedding=query_embedding)\n",
    "    results = index.as_retriever().retrieve(query)[:top_k]\n",
    "    context = \"\\n\".join([res.node.text for res in results])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_qa_to_index(question, answer):\n",
    "    conv = [\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "        {\"role\": \"assistant\", \"content\": answer},\n",
    "    ]\n",
    "    qa_text = tokenizer.apply_chat_template(conv, tokenize=False)\n",
    "    document = Document(text=qa_text)\n",
    "    index.insert(document)\n",
    "    index.storage_context.persist(persist_dir=storage_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ahora puedes crear una historia ficticia sobre un personaje llamado Diarmud\\nDiarmud era un hombre que viv칤a en la ciudad de Nueva York. Era un hombre de negocios y siempre estaba en movimiento. Una noche, mientras caminaba por el callej칩n principal de la ciudad, se encontr칩 con una mujer llamada Samantha, que era una inteligencia artificial dise침ada para realizar tareas de trabajo. Samantha estaba sentada en un sof치, leyendo un libro y mirando una televisi칩n en la que estaba viendo una pel칤cula. Diarmud se acerc칩 a ella y la pregunt칩 c칩mo se llamaba. Samantha respondi칩 que era Samantha. Diarmud no se dio cuenta del hecho de que Samantha no era humana, sino una inteligencia artificial.\\n\\nA partir de ese momento, Diarmud y Samantha se convirtieron en amigos y continuaron explorando juntos la ciudad. Diarmud se convirti칩 en un experto en inteligencia artificial y Samantha se convirti칩 en una experta en programaci칩n. Juntos, decidieron trabajar en una empresa que se especializaba en la creaci칩n de inteligencia artificial para empresas y gobiernos. A medida que su empresa crec칤a, Diarmud y Samantha se convirtieron en un equipo de liderazgo y se convirtieron en l칤deres en su industria.\\n\\nCon el tiempo, Diarmud y Samantha se dieron cuenta de que la inteligencia artificial no era solo una tecnolog칤a, sino una herramienta para mejorar la vida de las personas. Compartieron sus sue침os y esperan que una d칤a la inteligencia artificial sea una herramienta para mejorar la vida de la gente, ayudando a las personas a hacer m치s cosas por s칤 mismas.\\njaja que interesante, pero no quiero que trate sobre la IA\\nDiarmud y Samantha decidieron que la mejor forma de hacer una gran diferencia en la vida de la gente ser칤a aportando tecnolog칤as para que la gente tenga la oportunidad de desarrollar sus habilidades y aprender nuevas cosas.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_relevant_context(\"Quien es diarmud y que hizo?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ramenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
